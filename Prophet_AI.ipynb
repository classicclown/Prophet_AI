{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "543c140b-cb94-48d8-8aa6-819f6085eb72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Model reminder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1eced5c-4cfe-4550-ab59-28fe0defb68b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### NeuralProphet Forecasting Methods Guide\n",
    "\n",
    "#### Model Strategy\n",
    "- **Daily Model**: Forecast daily sales → aggregate to weeks/months\n",
    "- **Weekly Model**: Forecast weekly sales → aggregate to months/quarters\n",
    "- Maximum flexibility with daily base + validation from weekly model\n",
    "\n",
    "## Forecasting Methods\n",
    "\n",
    "#### 1. Simple .predict(test_df)\n",
    "```python\n",
    "forecast = m.predict(test_df)\n",
    "plot_data = forecast[['ds', 'yhat1']].dropna()  # Use yhat1 column\n",
    "```\n",
    "- **Use when**: You have test data with known dates\n",
    "- **Extract**: `yhat1` column (each prediction is 1-step ahead)\n",
    "- **Best for**: Model validation on historical test periods\n",
    "\n",
    "#### 2. make_future_dataframe() - Single Shot\n",
    "```python\n",
    "future_df = m.make_future_dataframe(train_df, periods=30, events=holidays_df)\n",
    "forecast = m.predict(future_df)\n",
    "\n",
    "# Extract diagonal values (Day 1=yhat1, Day 2=yhat2, etc.)\n",
    "forecast_clean = extract_diagonal_forecast(forecast, periods=30)\n",
    "```\n",
    "- **Use when**: Forecasting into future, periods ≤ model's n_forecasts\n",
    "- **Extract**: Diagonal approach (yhat1, yhat2, yhat3...)\n",
    "- **Best for**: Short to medium-term forecasting within model's native horizon\n",
    "\n",
    "#### 3. Recursive Function\n",
    "```python\n",
    "forecast = recursive_predict(m, train_df, forecast_periods=90, holidays_df=holidays_df)\n",
    "plot_data = forecast[['ds', 'yhat1']]  # Already clean format\n",
    "```\n",
    "- **Use when**: Need to forecast beyond model's n_forecasts parameter\n",
    "- **Extract**: Returns clean format with `ds` and `yhat1` columns\n",
    "- **Best for**: Long-term forecasting (90+ days)\n",
    "\n",
    "### Decision Tree\n",
    "\n",
    "**Do you have test data with known dates?**\n",
    "→ YES: Use `.predict(test_df)` + `yhat1`\n",
    "\n",
    "**Forecasting into unknown future?**\n",
    "→ Short term (≤ n_forecasts): Use `make_future_dataframe` + diagonal extraction\n",
    "→ Long term (> n_forecasts): Use `recursive_predict` + `yhat1`\n",
    "\n",
    "### Data Aggregation Examples\n",
    "\n",
    "#### Daily to Weekly/Monthly\n",
    "```python\n",
    "# From daily forecast\n",
    "weekly_agg = daily_forecast.set_index('ds').resample('W').sum().reset_index()\n",
    "monthly_agg = daily_forecast.set_index('ds').resample('M').sum().reset_index()\n",
    "```\n",
    "\n",
    "#### Weekly to Monthly/Quarterly  \n",
    "```python\n",
    "# From weekly forecast\n",
    "monthly_agg = weekly_forecast.set_index('ds').resample('M').sum().reset_index()\n",
    "quarterly_agg = weekly_forecast.set_index('ds').resample('Q').sum().reset_index()\n",
    "```\n",
    "\n",
    "### Model Setup Template\n",
    "\n",
    "#### Daily Model\n",
    "```python\n",
    "daily_params = {'n_lags': 30, 'quantiles': [0.05, 0.95], 'weekly_seasonality': True}\n",
    "daily_model = NeuralProphet(**daily_params)\n",
    "daily_model.add_events('Holiday')\n",
    "```\n",
    "\n",
    "#### Weekly Model  \n",
    "```python\n",
    "# Prepare weekly data\n",
    "weekly_df = daily_df.set_index('ds').resample('W').sum().reset_index()\n",
    "weekly_df.columns = ['ds', 'y']\n",
    "\n",
    "weekly_params = {'n_lags': 8, 'yearly_seasonality': True}\n",
    "weekly_model = NeuralProphet(**weekly_params)\n",
    "```\n",
    "\n",
    "### Key Reminders\n",
    "- **yhat1 vs Diagonal**: Use yhat1 for sequential predictions, diagonal for multi-step from single point\n",
    "- **Holidays**: Always include `holidays_df` parameter in forecasting functions\n",
    "- **n_forecasts**: Check your model's native forecast horizon to choose method\n",
    "- **Validation**: Compare daily→weekly vs weekly direct forecasts for accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed5e3df3-c2d0-4251-b4b3-a4bbbe87affe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "368c3417-3b50-4c4c-8bc9-d399186abe56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from neuralprophet import NeuralProphet, set_log_level\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "import warnings\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "import plotly.graph_objects as go\n",
    "from copy import deepcopy\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "set_log_level(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e5b6afc-9156-4f69-9db5-7e6760214323",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Model Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "49287be1-16d8-4c26-adfc-c7da71b66143",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def model_testing (test_df, test_forecast):\n",
    "  mse=np.sqrt(mean_squared_error(y_true=test_df['y'], y_pred=test_forecast['yhat1']))\n",
    "  mae=mean_absolute_error(y_true=test_df['y'], y_pred=test_forecast['yhat1'])\n",
    "  mape=mean_absolute_percentage_error(y_true=test_df['y'], y_pred=test_forecast['yhat1'])\n",
    "\n",
    "  print('Mean Squared Error:', mse)\n",
    "  print('Mean Absolute Error:', mae)\n",
    "  print('Mean Absolute Percentage Error:', mape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40e07246-a5da-4b10-8279-0b63f50b9e79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Import dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03aee984-b087-4a19-a10a-b01860e331e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Workspace/Repos/ryan@delve.systems/Prophet_AI/Wellness_Sales_Quantity_Grouped.csv')\n",
    "\n",
    "df.rename(columns = {'trandate':'ds', 'AvgSale':'y'}, inplace = True)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c1a74041-feeb-435d-b25e-34e67952e46d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add actual data\n",
    "fig.add_trace(go.Scatter(x=df['ds'], y=df['y'], mode='lines', name='Actual'))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Actuals',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Value'\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8c3756f-2f7d-42d4-abb5-eb3adc4d5bd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Data quality checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "913d36a6-2e2a-4943-a814-7e99aae015a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "63ab2704-6b7a-46d2-973f-2dd4607dc8bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark_df = spark.createDataFrame(df)\n",
    "consistency_check = spark_df.groupby('ds').count().orderBy('count')\n",
    "print (f\"Data consistency: {consistency_check.display()}\")\n",
    "\n",
    "if consistency_check.filter(consistency_check['count'] < 1).count() > 0:\n",
    "    print(\"Data consistency check failed: There are multiple records for the same date.\")\n",
    "else:\n",
    "    print(\"Data consistency check passed: All dates are unique.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e1ee837-1e44-477e-a5df-6212537e9ead",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Accuracy check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "bc450988-1640-4793-bd78-d92c78487ec0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "accuracy_check_expression = \"y < 0 OR y > 12000\"\n",
    "\n",
    "accuracy_check_result = spark_df.filter(accuracy_check_expression).count()\n",
    "\n",
    "if accuracy_check_result > 0:\n",
    "    print(f\"Accuracy check failed: {accuracy_check_result} records have values outside the range [0, 12000].\")\n",
    "else:\n",
    "    print(\"Accuracy check passed: All records have values within the range [0, 12000].\")\n",
    "\n",
    "spark_df[spark_df['y']>12000].display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f053326e-941a-4f15-86fd-5fa0520dd79c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f38c4a02-6895-4bff-a31a-c28fd41b019a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Detect outliers using z-score\n",
    "mean = spark_df.select(F.mean('y')).collect()[0][0]\n",
    "std = spark_df.select(F.stddev('y')).collect()[0][0]\n",
    "z_score_threshold = 5\n",
    "\n",
    "# Calculate z-score for each record \n",
    "df_with_z_score = spark_df.withColumn('z_score', (F.col(\"y\")-mean)/std)\n",
    "\n",
    "outliers = df_with_z_score.filter(~F.col('z_score').between(-z_score_threshold, z_score_threshold))\n",
    "cleaned_df = df_with_z_score.filter(F.col(\"z_score\").between(-z_score_threshold, z_score_threshold))\n",
    "# Filter outliers\n",
    "\n",
    "# Mark as outliers\n",
    "df_with_outlier = df_with_z_score.withColumn(\"_outlier\",\n",
    "    F.when(\n",
    "        (F.col(\"z_score\") < -z_score_threshold) |\n",
    "        (F.col(\"z_score\") > z_score_threshold), 1\n",
    "    ).otherwise(0))\n",
    "print(f\"With outliers - count: {spark_df.count()}\")\n",
    "print(f\"Global_active_power - mean: {mean}, stddev_value:{std}, z_score_threshold: {z_score_threshold}\")\n",
    "print(f\"Without outliers - count: {cleaned_df.count()}\")\n",
    "print(f\"Outliers - count: {outliers.count()}\")\n",
    "print(\"Outliers:\")\n",
    "outliers.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "443bf096-e533-4a55-8a0b-affe51883090",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "18544f50-bc28-499c-a8ce-3b5394d4777a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "min_max_values = cleaned_df.select( F.min(F.col('y')).alias(\"min_y\"),\n",
    "                                   F.max(F.col('y')).alias(\"max_y\")).collect()[0]\n",
    "\n",
    "# Normalize the columns\n",
    "min_value = min_max_values[f\"min_y\"]\n",
    "max_value = min_max_values[f\"max_y\"]\n",
    "\n",
    "normalized_df = cleaned_df.withColumn('normalized_y', (F.col('y') - min_value) / (max_value - min_value))\n",
    "normalized_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b93e508e-20ec-491b-9c1e-1118fd865d58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Standardizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a92c7111-2471-45df-bac3-c7c7f1b4ea55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Standardizing the dataset:\n",
    "#NeuralProphet uses gradient descent under the hood (like a neural network), and scaling helps with faster convergence and better stability.\n",
    "#Especially helpful if your 'y' values are large (e.g. in the thousands or more), or have seasonal variations on different scales.\n",
    "\n",
    "df_cleaned = cleaned_df.toPandas()\n",
    "\n",
    "log_col = np.log(df_cleaned['y'])\n",
    "mean_value = log_col.mean()\n",
    "std_value = log_col.std()\n",
    "\n",
    "log_stats = {'mean': mean_value, 'std': std_value}\n",
    "df_cleaned[f'standardized_y'] = (log_col - mean_value) / std_value\n",
    "df_cleaned['ds'] = pd.to_datetime(df_cleaned['ds'])\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aef474dc-ca1a-40c4-9a37-ea72cabe1e11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Data Profile with Ydata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "4ef30b0e-1d78-4279-b439-e41f95e65e67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "profile = ProfileReport(df_cleaned, title='Time Series Data Profiling',tsmode=True,sortby='ds',infer_dtypes=False,interactions=None,missing_diagrams=None,\n",
    "                        correlations={\"auto\":{\"calculate\":False},\n",
    "                                      \"pearson\": {\"calculate\": False},\n",
    "                                      \"spearman\": {\"calculate\": False}})\n",
    "\n",
    "profile.to_file(\"TimeSeriesProfiling.html\")\n",
    "report_html = profile.to_html()\n",
    "displayHTML(report_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4019fd2d-497b-4f1d-b9fb-2f3eff3d4cf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Check for Stationarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e902be75-bf96-47bc-92b2-d8044d2dea7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "result = adfuller(df_cleaned['y'])\n",
    "print(f'ADF Statistic: {result[0]}')\n",
    "print(f'p-value: {result[1]}')\n",
    "\n",
    "if result[1] < 0.05:\n",
    "    print(\"Reject the null hypothesis. The time series is stationary.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis. The time series is non-stationary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a6d771e3-3aa2-4256-86f5-e284fc08d55a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_cleaned['y_diff'] = df_cleaned['y'].diff().dropna()\n",
    "result = adfuller(df_cleaned['y_diff'].dropna())\n",
    "print(f'ADF Statistic (1st diff): {result[0]}')\n",
    "print(f'p-value: {result[1]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad2e2f74-3fd9-451d-8538-0af5a458b390",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Skewness and Kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "36a63e04-8c2b-42d8-9645-71de2de89579",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume df_cleaned is already a Pandas DataFrame and has a 'ds' column (datetime)\n",
    "df_cleaned['DayOfWeek'] = pd.to_datetime(df_cleaned['ds']).dt.dayofweek +1  # Monday = 0, Sunday = 6\n",
    "\n",
    "# Plot the distribution\n",
    "sns.histplot(df_cleaned['DayOfWeek'], kde=True, bins=7)\n",
    "plt.xlabel(\"Day of Week (0=Monday, 6=Sunday)\")\n",
    "plt.title(\"Distribution of Day of Week\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0242b05f-1eda-4aef-b29b-14c996ae39df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sns.boxplot(x=df_cleaned['DayOfWeek'], y=df_cleaned['y'])\n",
    "plt.xlabel(\"Day of Week (0=Monday, 6=Sunday)\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Boxplot of y by Day\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "210e8866-b0de-4db1-a680-878138e33f71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Autocorrelation and partial correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "dfe194e6-c8b5-446c-9bb0-0557199bae71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "acf = plot_acf(df_cleaned['y'], lags=3*24)\n",
    "pacf = plot_pacf(df_cleaned['y'], lags=3*24)\n",
    "plt.show()\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84ed067e-995d-4e9b-b96b-62b9058723ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "03cb93d1-b0b7-4567-a5f7-bae83602598f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Ensure 'ds' is datetime and 'y' is your target column\n",
    "# df_cleaned['ds'] = pd.to_datetime(df_cleaned['ds'])\n",
    "# df_cleaned.set_index('ds', inplace=True)\n",
    "\n",
    "# Decompose 'y' over a daily frequency\n",
    "results = seasonal_decompose(df_cleaned['y'], period = 30)  # weekly seasonality\n",
    "\n",
    "# Plot the result\n",
    "results.plot()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b424e91-71c0-4cd9-b02e-1736d9ce8e48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Create Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7740ecab-c2d4-4c03-80b7-1137dedb76d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Make sure ds is datetime\n",
    "df['ds'] = pd.to_datetime(df['ds'])\n",
    "\n",
    "# Split by date\n",
    "split_date = df_cleaned['ds'].max() - pd.Timedelta(days=90)\n",
    "\n",
    "train_df = df_cleaned[df_cleaned['ds'] < split_date]\n",
    "test_df = df_cleaned[df_cleaned['ds'] >= split_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "8e37e737-4823-47a4-b6ad-5f0dc7459bba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f95166d-bdb3-4f1c-a63e-f17a7e712acc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#SARIMA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71515f31-6709-4cd0-ac10-ba372d0265fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pmdarima as pm\n",
    "\n",
    "model = pm.auto_arima(train_df['y'],\n",
    "                       seasonal=True,\n",
    "                        m=7,\n",
    "                        d=0,\n",
    "                        D=1,\n",
    "                        max_p= 3,\n",
    "                        max_q=3,\n",
    "                        max_P=3,\n",
    "                        max_Q=3,\n",
    "                        information_criterion='aic',\n",
    "                        trace=True,\n",
    "                        stepwise=True,\n",
    "                        error_action='ignore', \n",
    "                        suppress_warnings=True)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea9f7997-0a82-48ea-94f5-2504d6ce0156",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def forecast_step ():\n",
    "  forecast, conf_int = model.predict(n_periods=1,\n",
    "                                    return_conf_int=True)\n",
    "  return (\n",
    "        forecast.tolist()[0],  # Convert forecast to scalar\n",
    "        np.asarray(conf_int).tolist()[0])  # Convert confidence interval to list\n",
    "  \n",
    "  \n",
    "  \n",
    "forecasts = []  # Store forecasts\n",
    "conf_intervals = []  # Store confidence intervals\n",
    "\n",
    "# Iterate over each observation in the test dataset\n",
    "for obs in test_df['y']:\n",
    "    forecast, conf_int = forecast_step()  # Forecast next step\n",
    "    forecasts.append(forecast)  # Append forecast to list\n",
    "    conf_intervals.append(conf_int)  # Append confidence interval to list\n",
    "\n",
    "    # Update the model with the new observation\n",
    "    model.update(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e23bc756-7469-4fbb-a9ff-3ea4b4ba7769",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from pmdarima.metrics import smape\n",
    "# Calculate and print the mean squared error of the forecasts\n",
    "print(f\"Mean squared error: {mean_squared_error(test_df['y'], forecasts)}\")\n",
    "# Calculate and print the Symmetric Mean Absolute Percentage Error (SMAPE)\n",
    "print(f\"SMAPE: {smape(test_df['y'], forecasts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29ab3e14-fec4-400c-aed0-5615862ab91e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Naive forecast means simply that yesterday = today\n",
    "naive_forecast = test_df['y'].shift(1)\n",
    "mse_naive = mean_squared_error(test_df['y'][1:], naive_forecast[1:])\n",
    "\n",
    "print(f\"Naive forecast MSE: {mse_naive}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65fad837-c9d7-42cf-9762-4bc237ffbbe3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set the figure size for the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Plot the test data\n",
    "plt.plot(test_df.index, test_df['y'], label='Test')\n",
    "# Plot the forecasted values\n",
    "plt.plot(test_df.index, forecasts, label='Forecast')\n",
    "# Label the x-axis as 'Date'\n",
    "plt.xlabel('Date')\n",
    "# Label the y-axis as 'Global_active_power'\n",
    "plt.ylabel('y')\n",
    "# Set the title of the plot\n",
    "plt.title('SARIMA Forecast vs Actuals')\n",
    "# Display the legend\n",
    "plt.legend()\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "283ab032-e380-4d57-8537-174815562ee4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Neural Prophet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "713ac8e1-83ce-467f-9d27-52bdfbbc22d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Build and train base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1694ea14-72c0-49b5-90ab-4c7e12b4da03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "m_base = NeuralProphet(\n",
    "    yearly_seasonality=True,\n",
    "    weekly_seasonality=True,\n",
    "    daily_seasonality=False,\n",
    ")\n",
    "\n",
    "metrics_base = m_base.fit(train_df)\n",
    "base_forecast = m_base.predict(test_df)\n",
    "base_forecast.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "27662db6-ca41-4fdd-9a9c-6e88053c8004",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plotting the test_df (actual values)\n",
    "#ax.plot(test_df['ds'], test_df['y'], label='Actual', color='blue')\n",
    "\n",
    "# Plotting the base_forecast (predicted values)\n",
    "ax.plot(base_forecast['ds'], base_forecast['yhat1'], label='Forecast', color='red')\n",
    "ax.plot(base_forecast['ds'], base_forecast['y'], label='Actual', color='blue')\n",
    "\n",
    "# Adding labels and title\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Value')\n",
    "ax.set_title('Actual vs. Forecasted Values')\n",
    "ax.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d596760d-5c8a-41d1-ad77-9d977e0e849c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_testing(test_df,base_forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a1af6c9-b637-4db4-8a31-2b7f8639eea9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6662981-fdd7-413b-a26a-2a2c66be683e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Build and train model with holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6feedd9f-a283-4ddf-9e3a-addfdd3f2d72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "holidays_df = pd.DataFrame({\n",
    "    \"event\": 'Holiday',\n",
    "    \"ds\": pd.to_datetime([\n",
    "        \"2023-03-21\",\n",
    "        \"2023-04-20\",\n",
    "        \"2023-04-21\",\n",
    "        \"2023-04-27\",\n",
    "        \"2023-05-01\",\n",
    "        \"2023-06-28\",\n",
    "        \"2023-06-29\",\n",
    "        \"2023-09-25\",\n",
    "        \"2023-12-01\",\n",
    "        \"2023-12-16\",\n",
    "        \"2023-12-25\",\n",
    "        \"2023-12-26\",\n",
    "        \"2024-01-01\",\n",
    "        \"2024-03-21\",\n",
    "        \"2024-03-29\",\n",
    "        \"2024-04-01\",\n",
    "        \"2024-04-27\",\n",
    "        \"2024-05-01\",\n",
    "        \"2024-06-16\",\n",
    "        \"2024-06-17\",\n",
    "        \"2024-08-09\",\n",
    "        \"2024-09-24\",\n",
    "        \"2024-12-16\",\n",
    "        \"2024-12-25\",\n",
    "        \"2024-12-26\",\n",
    "        \"2025-01-01\",  # New Year’s Day\n",
    "        \"2025-03-21\",  # Human Rights Day\n",
    "        \"2025-04-18\",  # Good Friday\n",
    "        \"2025-04-21\",  # Family Day\n",
    "        \"2025-04-27\",  # Freedom Day\n",
    "        \"2025-04-28\",  # Freedom Day Observed\n",
    "        \"2025-05-01\",  # Workers' Day\n",
    "        \"2025-06-16\",  # Youth Day\n",
    "        \"2025-08-09\",  # National Women’s Day\n",
    "        \"2025-09-24\",  # Heritage Day\n",
    "        \"2025-12-16\",  # Day of Reconciliation\n",
    "        \"2025-12-25\",  # Christmas Day\n",
    "        \"2025-12-26\"   # Day of Goodwill\n",
    "    ]),\n",
    "    \"lower_window\": 0,\n",
    "    \"upper_window\": 0\n",
    "})\n",
    "\n",
    "holidays_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f0878343-ae0d-4c78-8bc0-46d3d046ddef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "m_holidays = NeuralProphet(\n",
    "    yearly_seasonality=True,\n",
    "    weekly_seasonality=True,\n",
    "    daily_seasonality=False,\n",
    "    seasonality_mode='additive',\n",
    ")\n",
    "\n",
    "m_holidays.add_events('Holiday')\n",
    "\n",
    "# Create a combined dataframe with events\n",
    "df_with_holidays = m_holidays.create_df_with_events(train_df, holidays_df)\n",
    "\n",
    "# Fit the model with the combined dataframe\n",
    "metrics_holidays = m_holidays.fit(df_with_holidays)\n",
    "\n",
    "# For prediction, you need to include events in the test period too\n",
    "# First, filter holidays that fall within the test period\n",
    "test_start = test_df['ds'].min()\n",
    "test_end = test_df['ds'].max()\n",
    "test_holidays = holidays_df[(holidays_df['ds'] >= test_start) & (holidays_df['ds'] <= test_end)]\n",
    "\n",
    "# Create test dataframe with events\n",
    "test_with_holidays = m_holidays.create_df_with_events(test_df, test_holidays)\n",
    "\n",
    "# Make predictions\n",
    "base_forecast_holidays = m_holidays.predict(test_with_holidays)\n",
    "base_forecast_holidays.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d1a22ba-ec66-4a35-8276-50c4f63c7441",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plotting the test_df (actual values)\n",
    "ax.plot(test_df['ds'], test_df['y'], label='Actual', color='blue')\n",
    "\n",
    "# Plotting the base_forecast (predicted values)\n",
    "ax.plot(base_forecast_holidays['ds'], base_forecast_holidays['yhat1'], label='Forecast', color='red')\n",
    "# Adding labels and title\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Value')\n",
    "ax.set_title('Actual vs. Forecasted Values')\n",
    "ax.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d0c1c4f-9790-4904-a33b-3124959bd219",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_testing(test_df[0:89], base_forecast_holidays[0:89])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f136e43-df4f-4f82-ad22-76f7c154dd4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Build and train model with autoregression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1e611cbe-e1e9-4946-8eba-99dc8406e341",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "holidays_df = pd.DataFrame({\n",
    "    \"event\": 'Holiday',\n",
    "    \"ds\": pd.to_datetime([\n",
    "        \"2023-03-21\",\n",
    "        \"2023-04-20\",\n",
    "        \"2023-04-21\",\n",
    "        \"2023-04-27\",\n",
    "        \"2023-05-01\",\n",
    "        \"2023-06-28\",\n",
    "        \"2023-06-29\",\n",
    "        \"2023-09-25\",\n",
    "        \"2023-12-01\",\n",
    "        \"2023-12-16\",\n",
    "        \"2023-12-25\",\n",
    "        \"2023-12-26\",\n",
    "        \"2024-01-01\",\n",
    "        \"2024-03-21\",\n",
    "        \"2024-03-29\",\n",
    "        \"2024-04-01\",\n",
    "        \"2024-04-27\",\n",
    "        \"2024-05-01\",\n",
    "        \"2024-06-16\",\n",
    "        \"2024-06-17\",\n",
    "        \"2024-08-09\",\n",
    "        \"2024-09-24\",\n",
    "        \"2024-12-16\",\n",
    "        \"2024-12-25\",\n",
    "        \"2024-12-26\",\n",
    "        \"2025-01-01\",  # New Year’s Day\n",
    "        \"2025-03-21\",  # Human Rights Day\n",
    "        \"2025-04-18\",  # Good Friday\n",
    "        \"2025-04-21\",  # Family Day\n",
    "        \"2025-04-27\",  # Freedom Day\n",
    "        \"2025-04-28\",  # Freedom Day Observed\n",
    "        \"2025-05-01\",  # Workers' Day\n",
    "        \"2025-06-16\",  # Youth Day\n",
    "        \"2025-08-09\",  # National Women’s Day\n",
    "        \"2025-09-24\",  # Heritage Day\n",
    "        \"2025-12-16\",  # Day of Reconciliation\n",
    "        \"2025-12-25\",  # Christmas Day\n",
    "        \"2025-12-26\"   # Day of Goodwill\n",
    "    ]),\n",
    "    \"lower_window\": 0,\n",
    "    \"upper_window\": 0\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "308e09d8-dc90-4659-9ab8-7b0cae59f193",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df.head()\n",
    "\n",
    "#train_df_standardized = train_df[['ds','standardized_y']]\n",
    "#train_df = train_df[['ds','y']]\n",
    "test_df = test_df[['ds','y']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0edebf80-04ed-4858-90a7-d7578ef4cff4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###7 Day Forecast model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2df5ff08-d8fb-4bf2-9779-b73532cd7b6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df = train_df [['ds','y']]\n",
    "test_df = test_df[['ds','y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42b8086e-f902-4bb6-9b5a-5256e4a7f74c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a NeuralProphet model with default parameters\n",
    "# params = {\n",
    "#     'n_lags': 30,\n",
    "#     'n_forecasts': 7,\n",
    "#     'ar_reg': 0.18351866767708624,\n",
    "#     'seasonality_mode': 'additive',\n",
    "#     'yearly_seasonality': True,\n",
    "#     'weekly_seasonality': True,\n",
    "#     'daily_seasonality': False,\n",
    "# }\n",
    "\n",
    "params = {'n_lags':30,\n",
    "          'n_forecasts':7,\n",
    "        'quantiles':[0.05,0.95],\n",
    "        'weekly_seasonality': True}\n",
    "\n",
    "m = NeuralProphet(**params)\n",
    "m.add_events('Holiday')\n",
    "\n",
    "# Create a combined dataframe with events\n",
    "train_with_holidays = m.create_df_with_events(train_df, holidays_df)\n",
    "\n",
    "# Fit the model with the combined dataframe\n",
    "metrics = m.fit(train_with_holidays)\n",
    "\n",
    "#Filter holidays that fall within the test period\n",
    "test_start = test_df['ds'].min()\n",
    "test_end = test_df['ds'].max()\n",
    "test_holidays = holidays_df[(holidays_df['ds'] >= test_start) & (holidays_df['ds'] <= test_end)]\n",
    "\n",
    "# Create test dataframe with events\n",
    "test_with_holidays = m.create_df_with_events(test_df, test_holidays)\n",
    "\n",
    "# Make predictions for the duration of the test_with_holidays df\n",
    "forecast_holidays = m.predict(test_with_holidays)\n",
    "forecast_holidays.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c3a622e-067b-4fcf-a1cc-a1250327a33a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "future_df = m.make_future_dataframe(train_with_holidays, periods=7, events_df=holidays_df)\n",
    "forecast = m.predict(future_df)\n",
    "forecast.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56a5573e-2aa2-4ec9-86c9-2877f8ec8e3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Option 1: Use the diagonal values (proper multi-step forecast)\n",
    "forecast_values = []\n",
    "dates = []\n",
    "\n",
    "for i, row in forecast_holidays.iterrows():\n",
    "    # For each row, use the appropriate yhat column\n",
    "    # If this is the first forecast day, use yhat1\n",
    "    # If this is the second forecast day, use yhat2, etc.\n",
    "    \n",
    "    yhat_cols = [col for col in forecast_holidays.columns if col.startswith('yhat') and col[4:].replace('.', '').isdigit()]\n",
    "    \n",
    "    for j, col in enumerate(yhat_cols, 1):\n",
    "        if pd.notna(row[col]):\n",
    "            forecast_values.append(row[col])\n",
    "            dates.append(row['ds'])\n",
    "            break  # Use the first available forecast\n",
    "\n",
    "# Create clean forecast dataframe\n",
    "forecast_clean = pd.DataFrame({\n",
    "    'ds': dates,\n",
    "    'yhat1': forecast_values\n",
    "})\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(test_df['ds'], test_df['y'], label='Actual', color='blue')\n",
    "plt.plot(forecast_clean['ds'], forecast_clean['yhat1'], label='Forecast', color='red')\n",
    "plt.title('Actual vs. Forecasted Values')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1aa70ce5-db39-47ee-beef-0e4ee7946bb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_testing(test_df[-61: ], forecast_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3ab9c12-dba1-4ee1-860a-1454e355fc66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##30 Day Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fddc0f38-cfd4-419e-9abf-aaa7718df8f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split by date\n",
    "split_date = df_cleaned['ds'].max() - pd.Timedelta(days=120)\n",
    "\n",
    "train_df = df_cleaned[df_cleaned['ds'] < split_date]\n",
    "test_df = df_cleaned[df_cleaned['ds'] >= split_date]\n",
    "\n",
    "train_df = train_df[['ds','y']]\n",
    "test_df = test_df[['ds','y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfe59b8e-d71a-4a31-9e5f-536c6cda7f54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "params = {'n_lags':90,\n",
    "          'n_forecasts':30,\n",
    "        'quantiles':[0.05,0.95],\n",
    "        'weekly_seasonality': True}\n",
    "\n",
    "m_30 = NeuralProphet(**params)\n",
    "m_30.add_events('Holiday')\n",
    "\n",
    "# Create a combined dataframe with events\n",
    "train_with_holidays = m.create_df_with_events(train_df, holidays_df)\n",
    "\n",
    "# Fit the model with the combined dataframe\n",
    "metrics = m_30.fit(train_with_holidays)\n",
    "\n",
    "#Filter holidays that fall within the test period\n",
    "test_start = test_df['ds'].min()\n",
    "test_end = test_df['ds'].max()\n",
    "test_holidays = holidays_df[(holidays_df['ds'] >= test_start) & (holidays_df['ds'] <= test_end)]\n",
    "\n",
    "# Create test dataframe with events\n",
    "test_with_holidays = m_30.create_df_with_events(test_df, test_holidays)\n",
    "\n",
    "# Make predictions for the duration of the test_with_holidays df\n",
    "forecast_holidays = m_30.predict(test_with_holidays)\n",
    "forecast_holidays.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "13611ecf-40a1-4a95-9693-486e7733f38e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "future_df = m.make_future_dataframe(train_with_holidays, periods=1, events_df=holidays_df)\n",
    "forecast = m.predict(future_df)\n",
    "forecast.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0116621-759f-4dc3-b514-38b296038645",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Option 1: Use the diagonal values (proper multi-step forecast)\n",
    "forecast_values = []\n",
    "dates = []\n",
    "\n",
    "for i, row in forecast_holidays.iterrows():\n",
    "    # For each row, use the appropriate yhat column\n",
    "    # If this is the first forecast day, use yhat1\n",
    "    # If this is the second forecast day, use yhat2, etc.\n",
    "    \n",
    "    yhat_cols = [col for col in forecast_holidays.columns if col.startswith('yhat') and col[4:].replace('.', '').isdigit()]\n",
    "    \n",
    "    for j, col in enumerate(yhat_cols, 1):\n",
    "        if pd.notna(row[col]):\n",
    "            forecast_values.append(row[col])\n",
    "            dates.append(row['ds'])\n",
    "            break  # Use the first available forecast\n",
    "\n",
    "# Create clean forecast dataframe\n",
    "forecast_clean = pd.DataFrame({\n",
    "    'ds': dates,\n",
    "    'yhat1': forecast_values\n",
    "})\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "#plt.plot(test_df['ds'][-31:], test_df['y'][-31:], label='Actual', color='blue')\n",
    "plt.plot(forecast_clean['ds'], forecast_clean['yhat1'], label='Forecast', color='red')\n",
    "plt.title('Actual vs. Forecasted Values')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75fae70c-78f9-4bf7-913e-6ecc831c7283",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_testing(test_df[-31:], forecast_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a719a2c6-020e-4f3c-8e0c-5fc03991b5a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Recursive prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f901e10c-807b-4a0d-8ff0-e17a080794f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def recursive_predict_old(model, history_df, forecast_periods, n_forecasts=None):\n",
    "    \"\"\"\n",
    "    Recursively extend a NeuralProphet forecast beyond the native forecast horizon.\n",
    "\n",
    "    Args:\n",
    "        model: Trained NeuralProphet model.\n",
    "        history_df (pd.DataFrame): The historical training data with columns ['ds', 'y'].\n",
    "        forecast_periods (int): Total number of days to forecast into the future.\n",
    "        n_forecasts (int, optional): Number of steps forecasted per iteration.\n",
    "                                    If None, uses model.n_forecasts.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with dates and forecasted values.\n",
    "    \"\"\"\n",
    "    # Use model's n_forecasts if not specified\n",
    "    if n_forecasts is None:\n",
    "        # n_forecasts = params['n_forecasts']\n",
    "        n_forecasts = model.n_forecasts\n",
    "\n",
    "    # Create a copy of history to avoid modifying the original\n",
    "    df_history = deepcopy(history_df)\n",
    "\n",
    "    # Create a dataframe to store all forecasts\n",
    "    all_forecasts = pd.DataFrame(columns=['ds', 'yhat1'])\n",
    "\n",
    "    # Calculate how many iterations we need\n",
    "    remaining_periods = forecast_periods\n",
    "\n",
    "    while remaining_periods > 0:\n",
    "        print(f\"Forecasting {min(n_forecasts, remaining_periods)} days ahead...\")\n",
    "\n",
    "        # Generate future dataframe\n",
    "        periods = min(n_forecasts, remaining_periods)\n",
    "        df_future = model.make_future_dataframe(\n",
    "            df=df_history,\n",
    "            periods=periods,\n",
    "            n_historic_predictions=False,\n",
    "            events=holidays_df\n",
    "        )\n",
    "\n",
    "        # Predict\n",
    "        df_forecast = model.predict(df_future)\n",
    "\n",
    "        if df_forecast is None or df_forecast.empty:\n",
    "            print(f\"Warning: Empty forecast\")\n",
    "            break\n",
    "\n",
    "        # Check available forecast columns\n",
    "        forecast_cols = [col for col in df_forecast.columns if col.startswith('yhat')]\n",
    "        print(f\"Available forecast columns: {forecast_cols}\")\n",
    "\n",
    "        # Add forecasts to our collection\n",
    "        forecast_rows = []\n",
    "        new_history_rows = []\n",
    "\n",
    "        # If we only have yhat1 column but need to forecast multiple days\n",
    "        if len(forecast_cols) == 1 and forecast_cols[0] == 'yhat1':\n",
    "            # Use each row's yhat1 value\n",
    "            for i in range(len(df_forecast)):\n",
    "                forecast_row = {\n",
    "                    'ds': df_forecast['ds'].iloc[i],\n",
    "                    'yhat1': df_forecast['yhat1'].iloc[i]\n",
    "                }\n",
    "                forecast_rows.append(forecast_row)\n",
    "\n",
    "                # Also add to history for next iteration\n",
    "                new_history_row = {\n",
    "                    'ds': df_forecast['ds'].iloc[i],\n",
    "                    'y': df_forecast['yhat1'].iloc[i]\n",
    "                }\n",
    "                new_history_rows.append(new_history_row)\n",
    "        else:\n",
    "            # Handle multi-column case (yhat1, yhat2, etc.)\n",
    "            for j in range(min(len(forecast_cols), len(df_forecast))):\n",
    "                if j < len(df_forecast):\n",
    "                    yhat_col = forecast_cols[j]\n",
    "                    forecast_row = {\n",
    "                        'ds': df_forecast['ds'].iloc[j],\n",
    "                        'yhat1': df_forecast[yhat_col].iloc[j]\n",
    "                    }\n",
    "                    forecast_rows.append(forecast_row)\n",
    "\n",
    "                    new_history_row = {\n",
    "                        'ds': df_forecast['ds'].iloc[j],\n",
    "                        'y': df_forecast[yhat_col].iloc[j]\n",
    "                    }\n",
    "                    new_history_rows.append(new_history_row)\n",
    "\n",
    "        # Add to forecasts\n",
    "        if forecast_rows:\n",
    "            all_forecasts = pd.concat([all_forecasts, pd.DataFrame(forecast_rows)], ignore_index=True)\n",
    "\n",
    "            # Add to history for next iteration\n",
    "            if new_history_rows:\n",
    "                df_append = pd.DataFrame(new_history_rows)\n",
    "                df_history = pd.concat([df_history, df_append], ignore_index=True)\n",
    "\n",
    "                # Update remaining periods\n",
    "                remaining_periods -= len(new_history_rows)\n",
    "            else:\n",
    "                print(f\"Warning: No valid forecasts generated\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"Warning: No valid forecasts generated\")\n",
    "            break\n",
    "\n",
    "    # Limit to requested forecast periods\n",
    "    all_forecasts = all_forecasts.head(forecast_periods)\n",
    "\n",
    "    if len(all_forecasts) == 0:\n",
    "        print(\"No forecasts generated.\")\n",
    "        return None\n",
    "\n",
    "    return all_forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "acb5f863-309f-4fc1-a453-bec69767864d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "import warnings\n",
    "\n",
    "def recursive_predict(model, history_df, forecast_periods, n_forecasts=None, \n",
    "                     holidays_df=None, max_history_length=None):\n",
    "    \"\"\"\n",
    "    Recursively extend a NeuralProphet forecast beyond the native forecast horizon.\n",
    "\n",
    "    Args:\n",
    "        model: Trained NeuralProphet model.\n",
    "        history_df (pd.DataFrame): The historical training data with columns ['ds', 'y'].\n",
    "        forecast_periods (int): Total number of days to forecast into the future.\n",
    "        n_forecasts (int, optional): Number of steps forecasted per iteration.\n",
    "                                    If None, uses model.n_forecasts.\n",
    "        holidays_df (pd.DataFrame, optional): Holiday events dataframe.\n",
    "        max_history_length (int, optional): Maximum length of history to keep for memory efficiency.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with dates and forecasted values.\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If inputs are invalid.\n",
    "        RuntimeError: If forecasting fails.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input validation\n",
    "    if model is None:\n",
    "        raise ValueError(\"Model cannot be None\")\n",
    "    \n",
    "    if history_df is None or history_df.empty:\n",
    "        raise ValueError(\"history_df cannot be None or empty\")\n",
    "    \n",
    "    required_cols = ['ds', 'y']\n",
    "    if not all(col in history_df.columns for col in required_cols):\n",
    "        raise ValueError(f\"history_df must contain columns: {required_cols}\")\n",
    "    \n",
    "    if forecast_periods <= 0:\n",
    "        raise ValueError(\"forecast_periods must be positive\")\n",
    "    \n",
    "    # Check for NaN values\n",
    "    if history_df[required_cols].isnull().any().any():\n",
    "        warnings.warn(\"history_df contains NaN values, which may affect forecast quality\")\n",
    "    \n",
    "    # Use model's n_forecasts if not specified\n",
    "    if n_forecasts is None:\n",
    "        try:\n",
    "            n_forecasts = getattr(model, 'n_forecasts', 1)\n",
    "        except AttributeError:\n",
    "            n_forecasts = 1\n",
    "            warnings.warn(\"Could not determine model.n_forecasts, using 1\")\n",
    "    \n",
    "    if n_forecasts <= 0:\n",
    "        raise ValueError(\"n_forecasts must be positive\")\n",
    "    \n",
    "    # Create a copy (assuming data is already clean and sorted)\n",
    "    df_history = deepcopy(history_df)\n",
    "    \n",
    "    # Pre-allocate list for efficiency\n",
    "    all_forecasts = []\n",
    "    remaining_periods = forecast_periods\n",
    "    iteration = 0\n",
    "    max_iterations = forecast_periods  # Prevent infinite loops\n",
    "    \n",
    "    try:\n",
    "        while remaining_periods > 0 and iteration < max_iterations:\n",
    "            iteration += 1\n",
    "            periods_to_forecast = min(n_forecasts, remaining_periods)\n",
    "            \n",
    "            print(f\"Iteration {iteration}: Forecasting {periods_to_forecast} periods ahead...\")\n",
    "            \n",
    "            # Generate future dataframe\n",
    "            try:\n",
    "                df_future = model.make_future_dataframe(\n",
    "                    df=df_history,\n",
    "                    periods=periods_to_forecast,\n",
    "                    n_historic_predictions=False,\n",
    "                    events=holidays_df\n",
    "                )\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Failed to create future dataframe: {str(e)}\")\n",
    "            \n",
    "            # Predict\n",
    "            try:\n",
    "                df_forecast = model.predict(df_future)\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Prediction failed: {str(e)}\")\n",
    "            \n",
    "            if df_forecast is None or df_forecast.empty:\n",
    "                warnings.warn(f\"Empty forecast at iteration {iteration}\")\n",
    "                break\n",
    "            \n",
    "            # Extract forecast values more robustly\n",
    "            forecast_data = extract_forecast_values(df_forecast, periods_to_forecast)\n",
    "            \n",
    "            if not forecast_data:\n",
    "                warnings.warn(f\"No valid forecast data extracted at iteration {iteration}\")\n",
    "                break\n",
    "            \n",
    "            # Add to results\n",
    "            all_forecasts.extend(forecast_data)\n",
    "            \n",
    "            # Update history for next iteration\n",
    "            new_history_rows = [\n",
    "                {'ds': row['ds'], 'y': row['yhat1']} \n",
    "                for row in forecast_data\n",
    "            ]\n",
    "            \n",
    "            if new_history_rows:\n",
    "                df_new_history = pd.DataFrame(new_history_rows)\n",
    "                df_history = pd.concat([df_history, df_new_history], ignore_index=True)\n",
    "                \n",
    "                # Trim history if specified to manage memory\n",
    "                if max_history_length and len(df_history) > max_history_length:\n",
    "                    df_history = df_history.tail(max_history_length).reset_index(drop=True)\n",
    "                \n",
    "                remaining_periods -= len(new_history_rows)\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Recursive forecasting failed: {str(e)}\")\n",
    "    \n",
    "    # Convert to DataFrame and limit to requested periods\n",
    "    if not all_forecasts:\n",
    "        warnings.warn(\"No forecasts generated\")\n",
    "        return pd.DataFrame(columns=['ds', 'yhat1'])\n",
    "    \n",
    "    result_df = pd.DataFrame(all_forecasts)\n",
    "    result_df = result_df.head(forecast_periods)\n",
    "    \n",
    "    # Ensure proper data types\n",
    "    result_df['ds'] = pd.to_datetime(result_df['ds'])\n",
    "    result_df['yhat1'] = pd.to_numeric(result_df['yhat1'], errors='coerce')\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "\n",
    "def extract_forecast_values(df_forecast, expected_periods):\n",
    "    \"\"\"\n",
    "    Extract forecast values from prediction DataFrame more robustly.\n",
    "    \n",
    "    Args:\n",
    "        df_forecast (pd.DataFrame): Forecast results from model.predict()\n",
    "        expected_periods (int): Expected number of forecast periods\n",
    "        \n",
    "    Returns:\n",
    "        list: List of dictionaries with 'ds' and 'yhat1' keys\n",
    "    \"\"\"\n",
    "    forecast_data = []\n",
    "    \n",
    "    # Find available forecast columns\n",
    "    yhat_cols = [col for col in df_forecast.columns if col.startswith('yhat')]\n",
    "    \n",
    "    if not yhat_cols:\n",
    "        warnings.warn(\"No yhat columns found in forecast\")\n",
    "        return forecast_data\n",
    "    \n",
    "    # Get the last N rows (future predictions)\n",
    "    future_rows = df_forecast.tail(expected_periods)\n",
    "    \n",
    "    for _, row in future_rows.iterrows():\n",
    "        # Use the first available yhat column or yhat1 specifically\n",
    "        yhat_value = None\n",
    "        \n",
    "        if 'yhat1' in row and pd.notna(row['yhat1']):\n",
    "            yhat_value = row['yhat1']\n",
    "        elif yhat_cols and pd.notna(row[yhat_cols[0]]):\n",
    "            yhat_value = row[yhat_cols[0]]\n",
    "        \n",
    "        if yhat_value is not None and pd.notna(row['ds']):\n",
    "            forecast_data.append({\n",
    "                'ds': row['ds'],\n",
    "                'yhat1': float(yhat_value)\n",
    "            })\n",
    "    \n",
    "    return forecast_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebf3d7e9-22f3-42c2-9d7e-198c22292948",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "forecast_30day = recursive_predict(m, train_df, forecast_periods=30)\n",
    "print(forecast_30day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0959759f-3189-4995-a3eb-83f767db8741",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(forecast_30day['ds'], forecast_30day['yhat1'], label='Forecast', color='orange')\n",
    "plt.plot(test_df['ds'], test_df['y'], label='Actual', color='blue')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Forecasted Value')\n",
    "plt.title('90 Day Forecasted Timeline')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90bfaa66-9836-4218-8889-c3f13a51ea17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_testing(test_df[0:30],forecast_90day[0:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9be0f85-7f28-4105-aaea-cc282b6cb5ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33456ef9-43f3-4948-91be-affcc9ba6e52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "720e6e30-043f-47c8-8296-ec9a6057c74f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Exporting the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc32d8f8-0fcd-4721-84b7-6d0f1dab5cd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd77b335-157f-4c18-b8d5-fae5984a8bfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(\"/dbfs/tmp/\", exist_ok=True)\n",
    "\n",
    "class NeuralProphetWrapper(mlflow.pyfunc.PythonModel):\n",
    "    def __init__(self, model=None):\n",
    "        self.model = model\n",
    "        \n",
    "    def load_context(self, context):\n",
    "        # Load the pickled model\n",
    "        with open(context.artifacts[\"model_path\"], \"rb\") as f:\n",
    "            self.model = pickle.load(f)\n",
    "            \n",
    "        # Load holidays if they exist\n",
    "        if \"holidays_path\" in context.artifacts:\n",
    "            with open(context.artifacts[\"holidays_path\"], \"rb\") as f:\n",
    "                self.holidays_df = pickle.load(f)\n",
    "        else:\n",
    "            self.holidays_df = None\n",
    "            \n",
    "    def predict_old(self, context, model_input):\n",
    "        # Convert to DataFrame if it's not already\n",
    "        if not isinstance(model_input, pd.DataFrame):\n",
    "            model_input = pd.DataFrame(model_input)\n",
    "            \n",
    "        # Ensure 'ds' column exists and is datetime\n",
    "        if 'ds' not in model_input.columns:\n",
    "            raise ValueError(\"Input must contain 'ds' column with dates\")\n",
    "        \n",
    "        model_input['ds'] = pd.to_datetime(model_input['ds'])\n",
    "        \n",
    "        # For models with holidays, add holiday events\n",
    "        if hasattr(self, 'holidays_df') and self.holidays_df is not None:\n",
    "            # Filter holidays for the prediction period\n",
    "            pred_start = model_input['ds'].min()\n",
    "            pred_end = model_input['ds'].max()\n",
    "            relevant_holidays = self.holidays_df[\n",
    "                (self.holidays_df['ds'] >= pred_start) & \n",
    "                (self.holidays_df['ds'] <= pred_end)\n",
    "            ]\n",
    "            \n",
    "            if not relevant_holidays.empty:\n",
    "                # Add events to the dataframe\n",
    "                model_input = self.model.create_df_with_events(model_input, relevant_holidays)\n",
    "        \n",
    "        # Make prediction\n",
    "        try:\n",
    "            forecast = self.model.predict(model_input)\n",
    "            return forecast\n",
    "        except Exception as e:\n",
    "            print(f\"Prediction error: {e}\")\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        # Convert to DataFrame if it's not already\n",
    "        if not isinstance(model_input, pd.DataFrame):\n",
    "            model_input = pd.DataFrame(model_input)\n",
    "            \n",
    "        # Ensure 'ds' column exists and is datetime\n",
    "        if 'ds' not in model_input.columns:\n",
    "            raise ValueError(\"Input must contain 'ds' column with dates\")\n",
    "        \n",
    "        model_input['ds'] = pd.to_datetime(model_input['ds'])\n",
    "        \n",
    "        # Check if this is historical data (has 'y' column) or future data (no 'y' column)\n",
    "        has_y_values = 'y' in model_input.columns and model_input['y'].notna().any()\n",
    "        \n",
    "        # For models with holidays, add holiday events\n",
    "        if hasattr(self, 'holidays_df') and self.holidays_df is not None:\n",
    "            # Filter holidays for the prediction period\n",
    "            pred_start = model_input['ds'].min()\n",
    "            pred_end = model_input['ds'].max()\n",
    "            relevant_holidays = self.holidays_df[\n",
    "                (self.holidays_df['ds'] >= pred_start) & \n",
    "                (self.holidays_df['ds'] <= pred_end)\n",
    "            ]\n",
    "            \n",
    "            if not relevant_holidays.empty:\n",
    "                try:\n",
    "                    # Only add events if we have historical data OR if this is pure forecasting\n",
    "                    # For pure forecasting, we need to handle this differently\n",
    "                    if has_y_values:\n",
    "                        # Historical data with y values - safe to use create_df_with_events\n",
    "                        model_input = self.model.create_df_with_events(model_input, relevant_holidays)\n",
    "                    else:\n",
    "                        # Pure forecasting - manually add holiday columns\n",
    "                        model_input = self._add_holidays_manually(model_input, relevant_holidays)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not add events to dataframe: {e}\")\n",
    "                    # If events can't be added, proceed without them\n",
    "                    pass\n",
    "        \n",
    "        # Make prediction\n",
    "        try:\n",
    "            # For pure forecasting (no y values), we might need to handle differently\n",
    "            if not has_y_values and len(model_input) < 37:  # n_lags + n_forecasts\n",
    "                raise ValueError(\"Insufficient data for forecasting. Need at least 37 rows for this model.\")\n",
    "                \n",
    "            forecast = self.model.predict(model_input)\n",
    "            return forecast\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Prediction error: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def _add_holidays_manually(self, df, holidays_df):\n",
    "        \"\"\"\n",
    "        Manually add holiday columns without using create_df_with_events\n",
    "        which requires 'y' column\n",
    "        \"\"\"\n",
    "        # Create a copy to avoid modifying original\n",
    "        result_df = df.copy()\n",
    "        \n",
    "        # Add Holiday column, defaulting to 0\n",
    "        result_df['Holiday'] = 0\n",
    "        \n",
    "        # Mark holidays\n",
    "        holiday_dates = set(holidays_df['ds'].dt.date)\n",
    "        result_df.loc[result_df['ds'].dt.date.isin(holiday_dates), 'Holiday'] = 1\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    def make_future_dataframe_old(self, df, periods, freq='D'):\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            df = pd.DataFrame(df)\n",
    "            \n",
    "        if 'ds' not in df.columns:\n",
    "            raise ValueError(\"DataFrame must contain 'ds' column\")\n",
    "            \n",
    "        df['ds'] = pd.to_datetime(df['ds'])\n",
    "        last_date = df['ds'].max()\n",
    "        \n",
    "        # Create future dates\n",
    "        future_dates = pd.date_range(\n",
    "            start=last_date + pd.Timedelta(days=1),\n",
    "            periods=periods,\n",
    "            freq=freq\n",
    "        )\n",
    "        \n",
    "        # Create future dataframe\n",
    "        future_df = pd.DataFrame({'ds': future_dates})\n",
    "        \n",
    "        return future_df\n",
    "    \n",
    "    def make_future_dataframe(self, historical_df, periods, freq='D'):\n",
    "        \"\"\"\n",
    "        Create a dataframe for forecasting that includes historical context\n",
    "        This is the key method for proper forecasting\n",
    "        \"\"\"\n",
    "        if not isinstance(historical_df, pd.DataFrame):\n",
    "            historical_df = pd.DataFrame(historical_df)\n",
    "            \n",
    "        if 'ds' not in historical_df.columns:\n",
    "            raise ValueError(\"DataFrame must contain 'ds' column\")\n",
    "            \n",
    "        historical_df['ds'] = pd.to_datetime(historical_df['ds'])\n",
    "        last_date = historical_df['ds'].max()\n",
    "        \n",
    "        # For NeuralProphet with lags, we need historical context\n",
    "        # Get last 30 days of historical data (n_lags)\n",
    "        context_start = last_date - pd.Timedelta(days=29)  # 30 days total including last_date\n",
    "        historical_context = historical_df[\n",
    "            historical_df['ds'] >= context_start\n",
    "        ].copy()\n",
    "        \n",
    "        # Create future dates\n",
    "        future_dates = pd.date_range(\n",
    "            start=last_date + pd.Timedelta(days=1),\n",
    "            periods=periods,\n",
    "            freq=freq\n",
    "        )\n",
    "        \n",
    "        # Create future dataframe (no y values for pure forecasting)\n",
    "        future_df = pd.DataFrame({'ds': future_dates})\n",
    "        \n",
    "        # Combine historical context with future dates\n",
    "        forecast_df = pd.concat([\n",
    "            historical_context[['ds', 'y']],  # Include y for historical context\n",
    "            future_df  # No y for future dates\n",
    "        ], ignore_index=True)\n",
    "        \n",
    "        # Add holidays if available\n",
    "        if hasattr(self, 'holidays_df') and self.holidays_df is not None:\n",
    "            forecast_df = self._add_holidays_manually(forecast_df, self.holidays_df)\n",
    "        \n",
    "        return forecast_df\n",
    "\n",
    "# Save models using pickle\n",
    "def save_model(model, path):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    return path\n",
    "\n",
    "# Save models\n",
    "model_7_day_path = \"/dbfs/tmp/WellnessSalesForecast_7_day.pkl\"\n",
    "holidays_path = \"/dbfs/tmp/holidays_df.pkl\"\n",
    "\n",
    "# Save each model using pickle\n",
    "save_model(m, model_7_day_path)\n",
    "\n",
    "# Save holidays dataframe\n",
    "holidays_path = \"/dbfs/tmp/holidays_df.pkl\"\n",
    "with open(holidays_path, \"wb\") as f:\n",
    "    pickle.dump(holidays_df, f)\n",
    "\n",
    "# Function to register a model\n",
    "def register_model_old(model_path, model_name, model_type, holidays_path=None):\n",
    "    artifacts = {\"model_path\": model_path}\n",
    "    \n",
    "    # Add holidays path if needed\n",
    "    if model_type in [\"holidays\", \"autoregression\"] and holidays_path:\n",
    "        artifacts[\"holidays_path\"] = holidays_path\n",
    "    \n",
    "    with mlflow.start_run(run_name=f\"NeuralProphet_{model_type.capitalize()}_Model\") as run:\n",
    "        mlflow.pyfunc.log_model(\n",
    "            artifact_path=f\"neuralprophet_{model_type}_model\",\n",
    "            python_model=NeuralProphetWrapper(),\n",
    "            artifacts=artifacts,\n",
    "            conda_env={\n",
    "                \"channels\": [\"defaults\", \"conda-forge\"],\n",
    "                \"dependencies\": [\n",
    "                    \"python=3.8.0\",\n",
    "                    \"pip\",\n",
    "                    {\"pip\": [\n",
    "                        \"neuralprophet>=0.5.0\",\n",
    "                        \"pandas>=1.3.0\",\n",
    "                        \"numpy>=1.20.0\",\n",
    "                        \"matplotlib>=3.4.0\",\n",
    "                        \"plotly>=5.0.0\",\n",
    "                        \"torch>=1.9.0\"\n",
    "                    ]}\n",
    "                ],\n",
    "                \"name\": \"neuralprophet_env\"\n",
    "            }\n",
    "        )\n",
    "        run_id = run.info.run_id\n",
    "    \n",
    "    # Register the model\n",
    "    model_uri = f\"runs:/{run_id}/neuralprophet_{model_type}_model\"\n",
    "    model_details = mlflow.register_model(model_uri=model_uri, name=model_name)\n",
    "    \n",
    "    # Transition the model to production\n",
    "    client = MlflowClient()\n",
    "    client.transition_model_version_stage(\n",
    "        name=model_name,\n",
    "        version=model_details.version,\n",
    "        stage=\"Production\"\n",
    "    )\n",
    "    \n",
    "    return model_details\n",
    "\n",
    "def register_model(model_path, model_name, model_type, holidays_path=None):\n",
    "    artifacts = {\"model_path\": model_path}\n",
    "    \n",
    "    if holidays_path:\n",
    "        artifacts[\"holidays_path\"] = holidays_path\n",
    "    \n",
    "    with mlflow.start_run(run_name=f\"NeuralProphet_{model_type.capitalize()}_Model_Fixed\") as run:\n",
    "        mlflow.pyfunc.log_model(\n",
    "            artifact_path=f\"neuralprophet_{model_type}_model\",\n",
    "            python_model=NeuralProphetWrapper(),\n",
    "            artifacts=artifacts,\n",
    "            conda_env={\n",
    "                \"channels\": [\"defaults\", \"conda-forge\"],\n",
    "                \"dependencies\": [\n",
    "                    \"python=3.8.0\",\n",
    "                    \"pip\",\n",
    "                    {\"pip\": [\n",
    "                        \"neuralprophet>=1.0.0\",\n",
    "                        \"pandas>=1.3.0\",\n",
    "                        \"numpy>=1.20.0\",\n",
    "                        \"matplotlib>=3.4.0\",\n",
    "                        \"plotly>=5.0.0\",\n",
    "                        \"torch>=1.9.0\"\n",
    "                    ]}\n",
    "                ],\n",
    "                \"name\": \"neuralprophet_env\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Log model parameters\n",
    "        mlflow.log_params({\n",
    "            'n_lags': 30,\n",
    "            'n_forecasts': 7,\n",
    "            'weekly_seasonality': True,\n",
    "            'has_holidays': holidays_path is not None,\n",
    "            'quantiles': [0.05, 0.95],\n",
    "            'forecasting_method': 'with_historical_context',\n",
    "            'min_required_rows': 37\n",
    "        })\n",
    "        \n",
    "        run_id = run.info.run_id\n",
    "    \n",
    "    # Register the model\n",
    "    model_uri = f\"runs:/{run_id}/neuralprophet_{model_type}_model\"\n",
    "    model_details = mlflow.register_model(model_uri=model_uri, name=model_name)\n",
    "    \n",
    "    # Transition the model to production\n",
    "    client = MlflowClient()\n",
    "    client.transition_model_version_stage(\n",
    "        name=model_name,\n",
    "        version=model_details.version,\n",
    "        stage=\"Production\"\n",
    "    )\n",
    "    \n",
    "    return model_details\n",
    "\n",
    "\n",
    "# Register the model\n",
    "base_model_details = register_model(\n",
    "    model_7_day_path, \n",
    "    \"WellnessSalesForecast_7_day\", \n",
    "    \"holidays\",\n",
    "    holidays_path=holidays_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f196be2-0413-454c-bee6-e730fa2981c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Use model saved to ML Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a9451b9-8f41-449f-8530-bb7224b7d815",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "model_name = \"WellnessSalesForecast_7_Day\"\n",
    "model_stage = \"1\"  # or \"Staging\" or specific version like \"1\"\n",
    "\n",
    "model = mlflow.pyfunc.load_model(model_uri=f\"models:/{model_name}/{model_stage}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6c6d077-3478-406b-8a2a-473e2fd3f4cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example input for prediction\n",
    "future_df = pd.DataFrame({\n",
    "    \"ds\": pd.date_range(start=\"2025-06-10\", periods=30)\n",
    "})\n",
    "\n",
    "# Predict\n",
    "forecast = model.predict(test_df)\n",
    "forecast.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28b9d504-c585-4868-bd6e-d23f2335fcac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Using imported model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "135c3105-de3f-4f06-927f-ed8d3854a88f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d8aa672-1a3f-4c83-b451-a1f74b87eba2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "expirement_name = \"/Workspace/Users/ryan@delve.systems/FirstExperiment\"\n",
    "#mlflow.create_experiment(expirement_name)\n",
    "mlflow.set_experiment(expirement_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c935bfd-bb07-4123-a8da-b0041e08d93b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fddd83b8-321e-4919-8c0a-44410bf63022",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "with open('/Workspace/Users/ryan@delve.systems/Prophet_AI/neuralprophet_base.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "015362bb-4676-4604-a62c-8fb55885e7dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"model_type\", \"sklearn\")\n",
    "    mlflow.sklearn.log_model(model, \"model\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Prophet_AI",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
