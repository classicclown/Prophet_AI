{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84bb4b36-da6e-4977-bb85-cd4265ac3bbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64e15a43-feb7-418e-b0e1-9e52f17c22bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def connect_to_azure():\n",
    "    \"\"\"Establish and test connection to Azure SQL.\"\"\"\n",
    "    try:\n",
    "        # Connect to Azure SQL\n",
    "        spark = SparkSession.builder.getOrCreate()\n",
    "        dbutils = DBUtils(spark)\n",
    "        settings = Settings(dbutils)\n",
    "        logger.info(f\"Attempting to connect to Azure SQL\")\n",
    "        \n",
    "        # Get the connection\n",
    "        connection = get_connection()\n",
    "        \n",
    "        # Test the connection with a simple query\n",
    "        result = connection.execute_query(\"SELECT 1 AS test\")\n",
    "        if result and result.count() > 0:\n",
    "            logger.info(\"Azure Connection successful\")\n",
    "            return connection\n",
    "        else:\n",
    "            logger.error(\"Connection test failed.\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error connecting to Azure SQL: {str(e)}\")\n",
    "        import traceback\n",
    "        logger.error(traceback.format_exc())\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d58671dd-ba31-49bb-af28-5ee8cbdecd1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def push_to_azure(connection, resource_name, df, target_table=None, dry_run=False):\n",
    "    \"\"\"Push data to Azure SQL using direct SQL.\"\"\"\n",
    "    if df is None or df.empty:\n",
    "        logger.warning(f\"No data to push for {resource_name}\")\n",
    "        return 0\n",
    "    \n",
    "    table_name = target_table or resource_name\n",
    "    logger.info(f\"Pushing {len(df)} {resource_name} records to Azure SQL\")\n",
    "    \n",
    "    if dry_run:\n",
    "        logger.info(f\"DRY RUN: Would upsert {len(df)} rows to {table_name} table\")\n",
    "        return len(df)\n",
    "    else:\n",
    "        try:\n",
    "            # Import the necessary functions\n",
    "            from azure.azure_upsert import process_records_chunk\n",
    "            from schema.schema_definitions import SCHEMA\n",
    "            \n",
    "            # Find the primary key column(s)\n",
    "            resource_schema = SCHEMA[resource_name]\n",
    "            key_columns = []\n",
    "            \n",
    "            if \"fields\" in resource_schema:\n",
    "                for field_name, field_props in resource_schema[\"fields\"].items():\n",
    "                    if field_props.get(\"primaryKey\", False):\n",
    "                        key_columns.append(field_name)\n",
    "            \n",
    "            if not key_columns:\n",
    "                raise ValueError(f\"No primary key columns found for resource '{resource_name}'\")\n",
    "            \n",
    "            # Get the connection\n",
    "            engine = connection\n",
    "            \n",
    "            # Convert DataFrame to records\n",
    "            records = df.to_dict(orient='records')\n",
    "            \n",
    "            # Use process_records_chunk directly\n",
    "            batch_size = 10000  # Adjust as needed\n",
    "            result = process_records_chunk(records, resource_name, table_name, key_columns, engine, batch_size)\n",
    "            \n",
    "            rows_affected = result['upserted']\n",
    "            logger.info(f\"Successfully upserted {rows_affected} rows to {table_name} table\")\n",
    "            return rows_affected\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error upserting {resource_name} data: {str(e)}\")\n",
    "            import traceback\n",
    "            logger.error(traceback.format_exc())\n",
    "            return 0"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "push_to_azure",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
